{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f6ae20-b29a-4a54-bcc8-c8c7986968d6",
   "metadata": {},
   "source": [
    "### Author - Saurabh Mangal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ebc63-e612-421f-bfbf-9c52c29a5992",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd395e42-b009-49ac-b062-0e38ed992073",
   "metadata": {},
   "source": [
    "### Speaker Diaritization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbcf8c83-e3f3-487b-872e-65d5da5a4ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -dit-py-plugins (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dit-py-plugins (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dit-py-plugins (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dit-py-plugins (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dit-py-plugins (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dit-py-plugins (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !sudo apt-get install -y wkhtmltopdf                                                   # Installing wkhtmltopdf command-line tools.\n",
    "!pip install google-cloud-aiplatform google-cloud-speech --upgrade --quiet --user      # Installing Google Cloud SDK and Cloud Speech SDK\n",
    "!pip install PyPDF2 pdfkit --upgrade --quiet --user                    # Installing PDF handling tools\n",
    "# !pip install PyPDF2 py3-wkhtmltopdf pdfkit --upgrade --quiet --user                    # Installing PDF handling tools\n",
    "\n",
    "!pip install ratelimit backoff --upgrade --quiet --user    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c85a63a2-c1be-46e8-b9a2-bc81b8f744dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -dit-py-plugins (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dit-py-plugins (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dit-py-plugins (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dit-py-plugins (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dit-py-plugins (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dit-py-plugins (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !sudo apt-get install -y wkhtmltopdf     \n",
    "## Installing wkhtmltopdf command-line tools.\n",
    "# !pip install google-cloud-aiplatform google-cloud-speech --upgrade --quiet --user      # Installing Google Cloud SDK and Cloud Speech SDK\n",
    "# !pip install PyPDF2 py3-wkhtmltopdf pdfkit --upgrade --quiet --user                    # Installing PDF handling tools\n",
    "# !pip install ratelimit backoff --upgrade --quiet --user   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b544ba-5d3d-4483-89f0-a676e778518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This it the change -- USER  may have to do -- ##\n",
    "\n",
    "VERTEX_API_PROJECT = PROJECT_ID = \"my-project-0004-346516\" #'your-project' #@param {\"type\": \"string\"}\n",
    "VERTEX_API_LOCATION =REGION= 'us-central1' #@param {\"type\": \"string\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8606f9-39e8-4537-98fd-d14118ab9ddc",
   "metadata": {},
   "source": [
    "### Simple Speech to text using V1 with Diaratization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df2192a1-0563-4ba5-a968-c49fd0ae3978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'podcast_mu.m4a':\n",
      "  Metadata:\n",
      "    major_brand     : dash\n",
      "    minor_version   : 0\n",
      "    compatible_brands: iso6mp41\n",
      "    creation_time   : 2023-11-10T11:33:52.000000Z\n",
      "  Duration: 01:00:17.00, start: 0.000000, bitrate: 129 kb/s\n",
      "    Stream #0:0(eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 0 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-11-10T11:33:52.000000Z\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'podcast_out4.wav':\n",
      "  Metadata:\n",
      "    major_brand     : dash\n",
      "    minor_version   : 0\n",
      "    compatible_brands: iso6mp41\n",
      "    ISFT            : Lavf58.29.100\n",
      "    Stream #0:0(eng): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-11-10T11:33:52.000000Z\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      encoder         : Lavc58.54.100 pcm_s16le\n",
      "size=    2584kB time=00:00:15.00 bitrate=1411.2kbits/s speed= 590x    \n",
      "video:0kB audio:2584kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.002948%\n"
     ]
    }
   ],
   "source": [
    "# !ffmpeg -i podcast_mu.m4a podcast_out.wav\n",
    "\n",
    "# !ffmpeg -i podcast_mu.m4a -c:a aac -b:a 256k podcast_out2.aac\n",
    "\n",
    "# !ffmpeg -i podcast_mu.m4a -ss 00:00:05 -t 00:00:15 podcast_out4.wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c362b8c-9ba3-47f1-8e9a-3166b3db1636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation to complete...\n",
      "word: 'opening', speaker_tag: 1\n",
      "word: 'soal', speaker_tag: 1\n",
      "word: 'mu', speaker_tag: 1\n",
      "word: 'aja', speaker_tag: 1\n",
      "word: 'sering', speaker_tag: 1\n",
      "word: 'kita', speaker_tag: 1\n",
      "word: 'bosen', speaker_tag: 1\n",
      "word: 'itu', speaker_tag: 1\n",
      "word: 'kita', speaker_tag: 1\n",
      "word: 'butuh', speaker_tag: 1\n",
      "word: 'topik', speaker_tag: 1\n",
      "word: 'lain', speaker_tag: 1\n",
      "word: 'Hasilnya', speaker_tag: 1\n",
      "word: 'kayak', speaker_tag: 1\n",
      "word: 'begini', speaker_tag: 1\n",
      "word: 'masalah', speaker_tag: 1\n",
      "word: 'dibahas', speaker_tag: 1\n",
      "word: 'Apa', speaker_tag: 1\n",
      "word: 'perlu', speaker_tag: 1\n",
      "word: 'gue', speaker_tag: 1\n",
      "word: 'doain', speaker_tag: 1\n",
      "word: 'mu', speaker_tag: 1\n",
      "word: 'supaya', speaker_tag: 1\n",
      "word: 'menang', speaker_tag: 1\n",
      "word: 'terus', speaker_tag: 1\n",
      "Building transcriptions...\n",
      "\n",
      "opening soal mu aja sering kita bosen itu kita butuh topik lain Hasilnya kayak begini masalah dibahas Apa perlu gue doain mu supaya menang terus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import speech_v1p1beta1 as speech\n",
    "\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "speech_file = \"./podcast_out4.wav\"  ### This it the change -- USER  may have to do -- ##\n",
    "\n",
    "with open(speech_file, \"rb\") as audio_file:\n",
    "    content = audio_file.read()\n",
    "\n",
    "audio = speech.RecognitionAudio(content=content)\n",
    "\n",
    "diarization_config = speech.SpeakerDiarizationConfig(\n",
    "    enable_speaker_diarization=True,\n",
    "    min_speaker_count=2,\n",
    "    max_speaker_count=10,\n",
    ")\n",
    "\n",
    "config = speech.RecognitionConfig(\n",
    "    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "    # sample_rate_hertz=8000,\n",
    "    sample_rate_hertz=44100,\n",
    "    audio_channel_count = 2,\n",
    "    language_code=\"id-ID\",\n",
    "    diarization_config=diarization_config,\n",
    ")\n",
    "\n",
    "print(\"Waiting for operation to complete...\")\n",
    "response = client.recognize(config=config, audio=audio)\n",
    "\n",
    "# The transcript within each result is separate and sequential per result.\n",
    "# However, the words list within an alternative includes all the words\n",
    "# from all the results thus far. Thus, to get all the words with speaker\n",
    "# tags, you only have to take the words list from the last result:\n",
    "result = response.results[-1]\n",
    "\n",
    "words_info = result.alternatives[0].words\n",
    "\n",
    "# Printing out the output:\n",
    "for word_info in words_info:\n",
    "    print(f\"word: '{word_info.word}', speaker_tag: {word_info.speaker_tag}\")\n",
    "# return result\n",
    "\n",
    "\n",
    "print(\"Building transcriptions...\")\n",
    "transcript_builder = []\n",
    "# Each result is for a consecutive portion of the audio. Iterate through\n",
    "# them to get the transcripts for the entire audio file.\n",
    "for result in response.results:\n",
    "    alternative = result.alternatives[0]\n",
    "\n",
    "    # The first alternative is the most likely one for this portion.\n",
    "    transcript_builder.append(f\"\\n{result.alternatives[0].transcript}\")\n",
    "    # transcript_builder.append(f\"\\nConfidence: {result.alternatives[0].confidence}\")\n",
    "\n",
    "transcript = ''.join(transcript_builder)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1e1fe-cae7-45a0-8df7-62ea8e05e71c",
   "metadata": {},
   "source": [
    "## Speech to text using V1 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1542881f-d65b-42b1-b769-27a6cea0aff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "First alternative of result 0\n",
      "Transcript: the still smile of Wonders Water World that we should take time to the Bone\n",
      "Final response :::  the still smile of Wonders Water World that we should take time to the Bone\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import speech_v1p1beta1 as speech\n",
    "\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "# speech_file = \"resources/commercial_mono.wav\"\n",
    "\n",
    "with open(speech_file, \"rb\") as audio_file:\n",
    "    content = audio_file.read()\n",
    "\n",
    "audio = speech.RecognitionAudio(content=content)\n",
    "config = speech.RecognitionConfig(\n",
    "    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "    # sample_rate_hertz=8000,\n",
    "    sample_rate_hertz=44100,\n",
    "    audio_channel_count = 2,\n",
    "    language_code=\"id-ID\", #\"id-ID\" en-US\n",
    "    # use_enhanced=True,\n",
    "    # A model must be specified to use enhanced model.\n",
    "    # model=\"long\",\n",
    ")\n",
    "\n",
    "response = client.recognize(config=config, audio=audio)\n",
    "\n",
    "for i, result in enumerate(response.results):\n",
    "    alternative = result.alternatives[0]\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"First alternative of result {i}\")\n",
    "    print(f\"Transcript: {alternative.transcript}\")\n",
    "    fina_response = alternative.transcript\n",
    "\n",
    "print(\"Final response ::: \",fina_response)\n",
    "# return response.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f30c3e7-f454-497d-85a9-acd63ef497c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'(SQL)_data_eval_[NTK_Do_not_share_Externally].ipynb'\n",
      "'BigQuery One Pager.pdf'\n",
      "'Cost optimization best practices for BigQuery.pdf'\n",
      " LLM_Evaluation_Demo_Notebook_Vertex_Evaluation_for_LLM.ipynb\n",
      " Speech_to_text.ipynb\n",
      "'Untitled Folder'\n",
      " Untitled.ipynb\n",
      "'[external]_reinforcement_learning_from_ai_feedback_tutorial.ipynb'\n",
      " applied-ai-engineering-samples\n",
      " bahasa-diarization\n",
      " embedding-similarity-visualization.ipynb\n",
      " google_cloud_aiplatform-1.26.dev20230530+language.models.eval-py2.py3-none-any.whl\n",
      " google_cloud_aiplatform-1.38.dev20231206+generative.models-py2.py3-none-any.whl\n",
      " harvard.wav\n",
      " podcast_mu.m4a\n",
      " podcast_out.wav\n",
      " rlhf_pipeline.yaml\n",
      " shard-00000-of-00002.jsonl\n",
      " shard-00001-of-00002.jsonl\n",
      " stepback-qa.ipynb\n",
      " test_table_pdf.csv\n",
      " test_table_pdf.pdf\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c4e8062-26e7-4a21-a4bf-471e9d227db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation to complete...\n",
      "word: 'jangankan', speaker_tag: 1\n",
      "word: 'lo', speaker_tag: 1\n",
      "word: 'yang', speaker_tag: 1\n",
      "word: 'nonton', speaker_tag: 1\n",
      "word: 'dan', speaker_tag: 1\n",
      "word: 'bilang', speaker_tag: 1\n",
      "word: 'kita', speaker_tag: 1\n",
      "word: 'podcast', speaker_tag: 1\n",
      "word: 'mu', speaker_tag: 1\n",
      "word: 'bahasa', speaker_tag: 1\n",
      "word: 'mu', speaker_tag: 1\n",
      "word: 'terus', speaker_tag: 1\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import speech_v1p1beta1 as speech\n",
    "\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "# speech_file = \"./harvard.wav\"\n",
    "speech_file = './podcast_out3.wav'\n",
    "\n",
    "with open(speech_file, \"rb\") as audio_file:\n",
    "    content = audio_file.read()\n",
    "\n",
    "audio = speech.RecognitionAudio(content=content)\n",
    "\n",
    "diarization_config = speech.SpeakerDiarizationConfig(\n",
    "    enable_speaker_diarization=True,\n",
    "    min_speaker_count=2,\n",
    "    max_speaker_count=10,\n",
    ")\n",
    "\n",
    "config = speech.RecognitionConfig(\n",
    "    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "    # sample_rate_hertz=8000,\n",
    "    sample_rate_hertz=44100,\n",
    "    audio_channel_count = 2,\n",
    "    language_code=\"id-ID\",\n",
    "    model=\"latest_long\",\n",
    "    diarization_config=diarization_config,\n",
    ")\n",
    "\n",
    "print(\"Waiting for operation to complete...\")\n",
    "response = client.recognize(config=config, audio=audio)\n",
    "\n",
    "# The transcript within each result is separate and sequential per result.\n",
    "# However, the words list within an alternative includes all the words\n",
    "# from all the results thus far. Thus, to get all the words with speaker\n",
    "# tags, you only have to take the words list from the last result:\n",
    "result = response.results[-1]\n",
    "\n",
    "words_info = result.alternatives[0].words\n",
    "\n",
    "# Printing out the output:\n",
    "for word_info in words_info:\n",
    "    print(f\"word: '{word_info.word}', speaker_tag: {word_info.speaker_tag}\")\n",
    "# return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d0c769-0520-42e1-8227-1bfb539364bc",
   "metadata": {},
   "source": [
    "## Speech to text using V2 latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2f1633a-e291-4327-8f0e-266bb96e04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.speech_v2 import SpeechClient\n",
    "from google.cloud.speech_v2.types import cloud_speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15816dff-52f3-4d6b-9e1a-aa386023c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickstart_v2(\n",
    "    project_id: str,\n",
    "    audio_file: str,\n",
    ") -> cloud_speech.RecognizeResponse:\n",
    "    \"\"\"Transcribe an audio file.\"\"\"\n",
    "    # Instantiates a client\n",
    "    client = SpeechClient()\n",
    "\n",
    "    # Reads a file as bytes\n",
    "    with open(audio_file, \"rb\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    config = cloud_speech.RecognitionConfig(\n",
    "        auto_decoding_config=cloud_speech.AutoDetectDecodingConfig(),\n",
    "        language_codes=[\"id-ID\"],\n",
    "        model=\"long\",\n",
    "    )\n",
    "\n",
    "    request = cloud_speech.RecognizeRequest(\n",
    "        recognizer=f\"projects/{project_id}/locations/global/recognizers/_\",\n",
    "        config=config,\n",
    "        content=content,\n",
    "    )\n",
    "\n",
    "    # Transcribes the audio into text\n",
    "    response = client.recognize(request=request)\n",
    "\n",
    "    for result in response.results:\n",
    "        print(f\"Transcript: {result.alternatives[0].transcript}\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e0e20e38-afcf-41ab-95f6-8c0a4c25ad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: jangankan lo yang nonton dan bilang kita podcast mu bahasa mu terus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "results {\n",
       "  alternatives {\n",
       "    transcript: \"jangankan lo yang nonton dan bilang kita podcast mu bahasa mu terus\"\n",
       "    confidence: 0.904364884\n",
       "  }\n",
       "  result_end_offset {\n",
       "    seconds: 4\n",
       "    nanos: 920000000\n",
       "  }\n",
       "  language_code: \"id-ID\"\n",
       "}\n",
       "metadata {\n",
       "  total_billed_duration {\n",
       "    seconds: 5\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quickstart_v2(PROJECT_ID,speech_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cac9d3-6240-48c2-a5ae-44e132b49490",
   "metadata": {},
   "source": [
    "## USING V2 and Chrip model (need to create a recognizer \"bahasa-test\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b1040-c275-4dd0-ae2c-715eb0e02126",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_Bucket = \"genai-experiments-sm\" #@param {type:\"string\"}\n",
    "FILENAME = \"podcast_out3.wav\" #@param {type:\"string\"}\n",
    "# This section is a WIP, incorporate v2 for chirp (this limits times to <60 seconds)\n",
    "\n",
    "from google.cloud.speech_v2 import SpeechClient\n",
    "from google.cloud.speech_v2.types import cloud_speech\n",
    "\n",
    "gcs_uri = \"gs://\"+GCS_Bucket+\"/\"+FILENAME\n",
    "\n",
    "def transcribe_gcs_v2(\n",
    "    project_id: str,\n",
    "    recognizer_id: str,\n",
    "    gcs_uri: str,\n",
    ") -> cloud_speech.RecognizeResponse:\n",
    "    \"\"\"Transcribes audio from a Google Cloud Storage URI.\n",
    "\n",
    "    Args:\n",
    "        project_id: The GCP project ID.\n",
    "        recognizer_id: The ID of the recognizer.\n",
    "        gcs_uri: The Google Cloud Storage URI.\n",
    "\n",
    "    Returns:\n",
    "        The RecognizeResponse.\n",
    "    \"\"\"\n",
    "    # Instantiates a client\n",
    "    client = SpeechClient()\n",
    "\n",
    "    request = cloud_speech.CreateRecognizerRequest(\n",
    "        parent=f\"projects/{project_id}/locations/global\",\n",
    "        # parent=f\"projects/{project_id}/locations/asia-southeast1\",\n",
    "        recognizer_id=recognizer_id,\n",
    "        recognizer=cloud_speech.Recognizer(\n",
    "            language_codes=[\"id-ID\"], model=\"chirp\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Creates a Recognizer\n",
    "    operation = client.create_recognizer(request=request)\n",
    "    recognizer = operation.result()\n",
    "\n",
    "    config = cloud_speech.RecognitionConfig(auto_decoding_config={})\n",
    "\n",
    "    request = cloud_speech.RecognizeRequest(\n",
    "        recognizer=recognizer.name, config=config, uri=gcs_uri\n",
    "    )\n",
    "\n",
    "    # Transcribes the audio into text\n",
    "    response = client.recognize(request=request)\n",
    "\n",
    "    for result in response.results:\n",
    "        print(f\"Transcript: {result.alternatives[0].transcript}\")\n",
    "\n",
    "    return response\n",
    "\n",
    "# response = transcribe_gcs_v2(\"icm-testing-env\",\"testing123123123321\",gcs_uri)\n",
    "\n",
    "response = transcribe_gcs_v2(PROJECT_ID,\"bahasa-test2\",speech_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d10bd160-6bd8-4728-af08-2952d95d2b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9093f559-c9a0-4db2-a20c-49e72810305f",
   "metadata": {},
   "source": [
    "### USing Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa33173-485e-40c3-8832-802099920665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import VertexAI\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "from langchain.llms import VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ffcc4-068e-421d-b820-56641141a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT_ID = \"YOUR_PROJECT_ID_HERE\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "# Code examples may misbehave if the model is changed.\n",
    "MODEL_NAME = \"text-bison@001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ebb0a-3ff5-421d-93b2-9e293564e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at summarizing conversations. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        #few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "# llm = VertexAI(temperature=0.2)\n",
    "# llm = VertexAI(temperature=0.2,model_name='text-bison@001')\n",
    "\n",
    "llm = VertexAI(temperature=0.2,model_name='text-bison@latest')\n",
    "\n",
    "model = model_name = 'text-bison@latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004b095-f346-4ca3-a715-0f110e028ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(model, parameters, llm_call, show_activity = True):\n",
    "  response = model.predict(llm_call, **parameters).text\n",
    "\n",
    "  if show_activity:\n",
    "    BOLD = \"\\033[1m\"\n",
    "    UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
    "    print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
    "    print(f\"{BOLD}The response:{UNFORMAT}\")\n",
    "    print(response)\n",
    "\n",
    "  return response  # Return to `_` if not needed.\n",
    "\n",
    "\n",
    "# Wrap code cell output to improve notebook readability.\n",
    "# Source: https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results/61401455#61401455\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# def set_css():\n",
    "#   display(HTML('''\n",
    "#   <style>\n",
    "#     pre {\n",
    "#         white-space: pre-wrap;\n",
    "#     }\n",
    "#   </style>\n",
    "#   '''))\n",
    "# get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328dd99d-2d0a-49d1-b75a-d48541c6b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\" Conversation \"\"\" + transcript + \"\"\"\n",
    "summarize the convesation but combing the speaker tags \n",
    "A:\"\"\"\n",
    "\n",
    "_ = call_llm(model, parameters, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90107a64-83c5-48b4-a15b-7382cf16996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b8a07-dec0-41e9-8891-d7e121c84691",
   "metadata": {},
   "source": [
    "### Actions :-- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba9a82-e37c-42e3-9427-b066629409d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Trascribe bahasa\n",
    "## Compare accuracy \n",
    "## Using Gen AI - Summarize , extract insights \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa32f1-ced1-47c2-b5e7-0e6a5cf3591e",
   "metadata": {},
   "source": [
    "### NOtes\n",
    "\n",
    "https://cloud.google.com/speech-to-text/#\n",
    "\n",
    "https://console.cloud.google.com/vertex-ai/generative/speech/speech-to-text?hl=en_US\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf39ad-15ef-4f76-8663-16c0d7392fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "py312",
   "name": "pytorch-gpu.1-12.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m98"
  },
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
